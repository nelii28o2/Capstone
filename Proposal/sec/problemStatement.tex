\autsection{Problem Statement}{Samuel Rodríguez} %The problem explained

Most educational institutions manually grade coding assignments completed by
students \cite{Cheang2003}. Manual evaluation of code takes a great deal of time
and is prone to bad evaluations due to the substantial amount of different ways
a program can be written. This leads the grader to only evaluate the program's
output based on an specific input. Such a method of evaluation can be
maliciously exploited and does not measure code quality at all. Missing the
evaluation of code quality and other important details can lead professors to
evaluate students inconsistently. Currently, most professors and TAs evaluate
output by manually importing code into a workstation, most probably the grader's
own personal machine, and then manually executing the program through some test
cases. This is highly insecure because the code is not properly sandboxed
\footnote{In computer security, a sandbox is a security mechanism for separating
running programs. It is often used to execute untested code, or untrusted
programs.}. The code could be malicious and cause damage to the running
workstation or could even get a hold of very sensitive data without the consent
of the grader.

Jesús E. Luzón \cite{Chiki}, a TA from the University of Puerto Rico at
Mayagüez, comments when asked about the time it takes to grade a 50 line coding
assignment: \begin{quote} ``The way I grade the assignments is by creating a
testing project that only includes the student's code. I check out the project
from the students repository and then run the code and compare the output. For a
well written assignment, It takes me about 3 to 4 minutes to evaluate it. But
for an assignment that has errors, it takes me about double the time: from 6 to
8 minutes. This happens because I need to figure out where exactly the error
comes from, so that I can give the student some useful feedback." \end{quote}
Luzón grades an assigment weekly. When asked about how many students he grades,
he comments: \begin{quote} ``I have 23 students, which doesn't seem like too
many, but the time it takes to grade their assignments adds up quickly. In the
worst case, I can spend 3 hours grading a single assignment." \end{quote}
The proposed project will solve four problems that manual grading has:
\begin{itemize}
\item Time consuming grading process.
\item Slow and ineffective communication between students and graders.
\item Insecure evaluation because submitted code might be malicious.
\item Absence of code quality evaluation.
\end{itemize}

\subsection{Time}

Grading programming assignments is time-consuming and tedious, graders have many
examinations to do and eventually the human factor gets involved. As the process
keeps getting longer, the human becomes tired and his effectiveness to grade is
reduced \cite{Cheang2003}. This directly affects the grades of the assignments.
The proposed solution solves this problem by automation. The process of
verifying a code's correctness is done automatically by compiling the code and
then running it against the grader's specified test cases. If the code does not
compile, or if a test case fails, the submission is deemed as incorrect.

\subsection{Communication}

Due to the grading process, undesirable trends have been seen. Students do not
usually seek their TA for consulting due to the lack of feedback given in the
grading of the assignments. Moreover, if any feedback is given, it is less
effective because by the time the assignments are evaluated these are no longer
fresh in the student's mind \cite{Cheang2003}. The solution solves this problem
by providing in-line and general commenting on graded assignments. The grading
personnel is notified when an assignment has been submitted so that they can
quickly provided feedback. This opens up the possibility to give another chance
to the student to submit his or her work.

\subsection{Security}

Manual grading requires the students to e-mail a soft copy of the assignment
with an optional printed documentation containing the source code. If the TA
decides to import the student's program, he will be exposing his computer to
malicious code. The proposed solutions intends to solve this problem by running
the code in a contained environment. A students code will be executed with
limited privileges, time and memory.

\subsection{Code Quality}

The process of manual evaluation of assignments leads to some verification of
the code's quality, specifically maintainability; some graders observe for
structured code, logical variables names and good documentation. However, manual
evaluation might miss other aspects of code quality: performance and robustness.
The proposed solution will do code quality evaluation to some extent.
Performance will be tested by providing time and memory usage limits. Robustness
will be evaluated by testing the student's code against extreme cases that are
specified by the instructor. Some aspects of maintainability will be
automatically verified with the use of linters \footnote{In computer
programming, lint is the name given to a particular program that flags
suspicious usage in software written in any computer language. Lint-like tools
generally perform static analysis of source code. Lint as a term can also refer
more broadly to syntactic discrepancies in general.}. They include configurable
coding guidelines, the presence of documentation in the source code, and
practices that avoid the creation of hard to debug problems (i.e reporting
strict mode warnings).

Some aspects of code quality will not be addressed, including time complexity
analysis (i.e. Big-O complexity), implementation patterns (e.g. recursive vs.
iterative, top-down vs. bottom-up, etc.), logical naming of variables, relevancy of
comments, object oriented structure, and code security (i.e. finding exploits in
the submitted program).
